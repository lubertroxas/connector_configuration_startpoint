
  iceberg: |
    connector.name=delta_lake

    # If using HMS as Metastore
    hive.metastore.uri=thrift://hive:9083
    # hive.metastore=<thrift or thrift-cdp7> #only use thrift-cdp7 if connecting to CDP7.x
    # If setting up with authentication:
    # hive.metastore.authentication.type=<NONE or KERBEROS>
    # hive.metastore.thrift.impersonation.enabled=false
    # hive.metastore.service.principal=<user/host@DOMAIN>
    # hive.metastore.client.principal=<user@DOMAIN>
    # hive.metastore.client.keytab=</tmp/keytabs/my.keytab>
    # hive.metastore.thrift.catalog-name=<hive>

    # # If using glue as metastore
    # hive.metastore=glue
    # hive.metastore.glue.region=<AWS Region>
    # ## If using IAM role for Glue authentication
    # hive.metastore.glue.iam-role=arn:aws:iam::188806360106:role/glue-iceberg
    # ## If using EKS Kubernetes service account for Glue authentication
    # hive.metastore.glue.use-web-identity-token-credentials-provider=true
    # ## If using access key and secret key for authentication
    # hive.metastore.glue.aws-access-key=<Access Key>
    # hive.metastore.glue.aws-secret-key=<Secret Key>

    # # If using Unity Catalog
    # hive.metastore=unity
    # delta.security=read_only
    # hive.metastore.unity.host=<host>
    # hive.metastore.unity.token=<token>
    # hive.metastore.unit.catalog-name=<main/catalog name in Databricks>

    # File System Access - Choose and configure the appropriate one(s)

    # Hadoop  ### Check the correct properties
    # fs.hadoop.enabled = true # Enable HDFS access if needed
    # hive.config.resources=<Comma separated list of Hadoop configuration files> # e.g. /etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
    # hive.hdfs.authentication.type=KERBEROS # If using Kerberos authentication
    # hive.hdfs.impersonation.enabled=true # Enable impersonation for HDFS access
    # hive.hdfs.trino.principal=<Principal>
    # hive.hdfs.trino.keytab=<Path to Keytab>

    # S3 
    # fs.native-s3.enabled = true
    # s3.endpoint=<S3 Endpoint. Required for S3>
    # s3.region=<S3 Region. Required for S3>
    # s3.aws-access-key=<Access Key used for authentication>
    # s3.aws-secret-key=<AWS Secret used for authentication>

    # ADLS
    # fs.native-azure.enabled = true
    # azure.auth-type=ACCES_KEY #or OAuth2s
    # azure.endpoin= <ADLS Endpoint>
    # if using Access KEy
    # azure.access-key
    # If using OAuth2 for authentication to filesystem (ADLS)
    # azure.oauth.tenant-id=
    # azure.oauth.endpoint=
    # azure.oauth.client-id=
    # azure.oauth.secret=

    # GCS
    # fs.gcs.enabled = true
    # gcs.project-id=<Google Project ID>
    # gcs.endpoint=<Google Storage Endpoint if using custom URL>
    # # If using access token for authentication
    # gcs.use-access-token=true
    # # If providing key directly
    # gcs.json-key=<Service account key in JSON format>
    # # If key is in a file
    # gcs.json-key-file-path=<File path to json file containing key>
    
    # Catalog Redirects
    # iceberg.hive-catalog-name=<hive_catalog>>

    # Enable register table for external tables:
    iceberg.register-table-procedure.enabled=true

    # # File Format & Compression Settings if Needed: ZSTD are defaults and recommended
    delta.compression-codec = ZSTD 

    # # Statistics: Enabled by default, crucial for CBO 
    # iceberg.table-statistics-enabled = true 
    # iceberg.extended-statistics.enabled = true 
    # iceberg.extended-statistics.collect-on-write = true 

    # # Caching: Metadata cache enabled by default on coordinator
    iceberg.metadata-cache.enabled = true

    # ORC Bloom Filters can be enabled for ORC tables (requires table property)
    # orc.bloom-filters.enabled=true # Example ORC Bloom Filter enablement (from Hive context, applicable concept)
        
    # Maintenance Defaults (Important to understand minimums)
    iceberg.expire-snapshots.min-retention = 7d
    iceberg.remove-orphan-files.min-retention = 7d

    # Security Configuration
    iceberg.security=system
    # iceberg.security=read_only ## When connecting to Databricks using REST Catalog

    # MV configuration
    # materialized-views.enabled=true
    # materialized-views.namespace=catalog_hive_namespace
    # materialized-views.storage-schema=mv_cache_storage
s