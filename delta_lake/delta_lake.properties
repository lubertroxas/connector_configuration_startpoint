  delta_lake: |
    connector.name=delta_lake

    # If using Starburst data catalog as Metastore
    # hive.metastore=glue
    # hive.metastore.glue.endpoint-url=http://starburst-portal:8080/api/v1/glue
    # hive.metastore.glue.region=us-east-1
    # hive.metastore.glue.catalogid=<catalog_id>
    # hive.metastore.glue.aws-access-key=<accesskey>
    # hive.metastore.glue.aws-secret-key=<secretkey>

    # If using HMS as Metastore
    # hive.metastore.uri=thrift://hive:9083
    # hive.metastore=<thrift or thrift-cdp7> #only use thrift-cdp7 if connecting to CDP7.x
    # If setting up with authentication:
    # hive.metastore.authentication.type=<NONE or KERBEROS>
    # hive.metastore.thrift.impersonation.enabled=false
    # hive.metastore.service.principal=<user/host@DOMAIN>
    # hive.metastore.client.principal=<user@DOMAIN>
    # hive.metastore.client.keytab=</tmp/keytabs/my.keytab>
    # hive.metastore.thrift.catalog-name=<hive>

    # # If using glue as metastore
    # hive.metastore=glue
    # hive.metastore.glue.region=<AWS Region>
    # ## If using IAM role for Glue authentication
    # hive.metastore.glue.iam-role=arn:aws:iam::188806360106:role/glue-iceberg
    # ## If using EKS Kubernetes service account for Glue authentication
    # hive.metastore.glue.use-web-identity-token-credentials-provider=true
    # ## If using access key and secret key for authentication
    # hive.metastore.glue.aws-access-key=<Access Key>
    # hive.metastore.glue.aws-secret-key=<Secret Key>

    # # If using Unity Catalog
    # hive.metastore=unity
    # delta.security=read_only
    # hive.metastore.unity.host=<host>
    # hive.metastore.unity.token=<token>
    # hive.metastore.unit.catalog-name=<main/catalog name in Databricks>

    # File System Access - Choose and configure the appropriate one(s)
    # Hadoop  ### Check the correct properties
    # fs.hadoop.enabled = true # Enable HDFS access if needed
    # hive.config.resources=<Comma separated list of Hadoop configuration files> # e.g. /etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
    # hive.hdfs.authentication.type=KERBEROS # If using Kerberos authentication
    # hive.hdfs.impersonation.enabled=true # Enable impersonation for HDFS access
    # hive.hdfs.trino.principal=<Principal>
    # hive.hdfs.trino.keytab=<Path to Keytab>

    # S3 
    # fs.native-s3.enabled = true
    # s3.endpoint=<S3 Endpoint. Required for S3>
    # s3.region=<S3 Region. Required for S3>
    # s3.aws-access-key=<Access Key used for authentication>
    # s3.aws-secret-key=<AWS Secret used for authentication>

    # ADLS
    # fs.native-azure.enabled = true
    # azure.auth-type=ACCESS_KEY
    # azure.access-key=<Access Key used for authentication>
    # azure.endpoint=<ADLS Endpoint>
    # If using OAuth2 for authentication to filesystem (ADLS)
    # azure.auth-type=OAUTH2
    # azure.oauth.tenant-id=
    # azure.oauth.endpoint=
    # azure.oauth.client-id=
    # azure.oauth.secret=

    # GCS
    # fs.gcs.enabled = true
    # gcs.project-id=<Google Project ID>
    # gcs.endpoint=<Google Storage Endpoint if using custom URL>
    # # If using access token for authentication
    # gcs.use-access-token=true
    # # If providing key directly
    # gcs.json-key=<Service account key in JSON format>
    # # If key is in a file
    # gcs.json-key-file-path=<File path to json file containing key>
    

    # Delta Lake Settings
    delta.security=starburst
    delta.register-table-procedure.enabled=true
    delta.enable-non-concurrent-writes=true
    delta.extended-statistics.collect-on-write=false

    # File Format & Compression Settings if Needed: ZSTD are defaults and recommended
    delta.compression-codec = ZSTD 

    # MV configuration
    # materialized-views.enabled=true
    # materialized-views.namespace=catalog_hive_namespace
    # materialized-views.storage-schema=mv_cache_storage