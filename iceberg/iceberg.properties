  iceberg: |
    connector.name=iceberg

    # If using HMS as Metastore
    iceberg.catalog.type=hive_metastore 
    hive.metastore.uri=thrift://hive:9083
    # hive.metastore=<thrift or thrift-cdp7> #only use thrift-cdp7 if connecting to CDP7.x
    # If setting up with authentication:
    # hive.metastore.authentication.type=<NONE or KERBEROS>
    # hive.metastore.thrift.impersonation.enabled=false
    # hive.metastore.service.principal=<user/host@DOMAIN>
    # hive.metastore.client.principal=<user@DOMAIN>
    # hive.metastore.client.keytab=</tmp/keytabs/my.keytab>
    # hive.metastore.thrift.catalog-name=<hive>

    # # If using glue as metastore
    # iceberg.catalog.type=glue
    # hive.metastore.glue.region=<AWS Region>
    # ## If using IAM role for Glue authentication
    # hive.metastore.glue.iam-role=arn:aws:iam::188806360106:role/glue-iceberg
    # ## If using EKS Kubernetes service account for Glue authentication
    # hive.metastore.glue.use-web-identity-token-credentials-provider=true
    # ## If using access key and secret key for authentication
    # hive.metastore.glue.aws-access-key=<Access Key>
    # hive.metastore.glue.aws-secret-key=<Secret Key>

    # If using REST catalogs
    # iceberg.catalog.type=rest
    # iceberg.rest-catalog.uri=<Rest Catalog URI>
    # iceberg.rest-catalog.warehouse=<Warehouse identifier/location for the catalog (optional)>
    # iceberg.rest-catalog.security=<NONE or OAUTH2>
    ## If using OAUTH2 for authentication to iceberg REST catalog
    # iceberg.rest-catalog.oauth2.credential=<Credential>
    # iceberg.rest-catalog.oauth2.scope=<Scope>
    # ### OR ####
    # iceberg.rest-catalog.oauth2.token=<Token>

    # If using NESSIE catalog
    # iceberg.catalog.type=nessie
    # iceberg.nessie-catalog.uri=<Nessie Catalog URI>
    # iceberg.nessie-catalog.ref=<Optional defaults to main>
    # iceberg.nessie-catalog.default-warehouse-dir=<Default warehouse for schemas without location>
    # ## If using BEARER authentication to NESSIE catalog, default is no authentication
    # iceberg.nessie-catalog.authentication.type=BEARER
    # iceberg.nessie-catalog.authentication.token=<Bearer Token>


    # # If using Starburst Data Catalog as Metastore:
    # connector.name=iceberg
    # iceberg.catalog.type=glue_v2
    # hive.metastore.glue.endpoint-url=http://starburst-portal:8080/api/v1/glue
    # hive.metastore.glue.region=us-east-1
    # hive.metastore.glue.catalogid=<catalog-namespace>
    # hive.metastore.glue.aws-access-key=<sep-access-key>
    # hive.metastore.glue.aws-secret-key=<sep-secret-key>

    # File System Access - Choose and configure the appropriate one(s)

    # Hadoop  ### Check the correct properties
    # fs.hadoop.enabled = true # Enable HDFS access if needed
    # hive.config.resources=<Comma separated list of Hadoop configuration files> # e.g. /etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml
    # hive.hdfs.authentication.type=KERBEROS # If using Kerberos authentication
    # hive.hdfs.impersonation.enabled=true # Enable impersonation for HDFS access
    # hive.hdfs.trino.principal=<Principal>
    # hive.hdfs.trino.keytab=<Path to Keytab>

    # S3 
    # fs.native-s3.enabled = true
    # s3.endpoint=<S3 Endpoint. Required for S3>
    # s3.region=<S3 Region. Required for S3>
    # s3.aws-access-key=<Access Key used for authentication>
    # s3.aws-secret-key=<AWS Secret used for authentication>

    # ADLS
    # fs.native-azure.enabled = true
    # azure.auth-type=ACCESS_KEY 
    # azure.access-key=<Access Key>
    # azure.endpoint= <ADLS Endpoint>
    # If using OAuth2 for authentication to filesystem (ADLS)
    # azure.auth-type=OAUTH2 
    # azure.oauth.tenant-id=
    # azure.oauth.endpoint=
    # azure.oauth.client-id=
    # azure.oauth.secret=

    # GCS
    # fs.gcs.enabled = true
    # gcs.project-id=<Google Project ID>
    # gcs.endpoint=<Google Storage Endpoint if using custom URL>
    # # If using access token for authentication
    # gcs.use-access-token=true
    # # If providing key directly
    # gcs.json-key=<Service account key in JSON format>
    # # If key is in a file
    # gcs.json-key-file-path=<File path to json file containing key>
    
    # Catalog Redirects
    # iceberg.hive-catalog-name=<hive_catalog>>

    # Enable register table for external tables:
    iceberg.register-table-procedure.enabled=true

    # # File Format & Compression Settings if Needed: PARQUET and ZSTD are defaults and recommended
    iceberg.file-format = PARQUET 
    iceberg.compression-codec = ZSTD 

    # # Statistics: Enabled by default, crucial for CBO 
    # iceberg.table-statistics-enabled = true 
    # iceberg.extended-statistics.enabled = true 
    # iceberg.extended-statistics.collect-on-write = true 

    # # Caching: Metadata cache enabled by default on coordinator
    iceberg.metadata-cache.enabled = true

    # ORC Bloom Filters can be enabled for ORC tables (requires table property)
    # orc.bloom-filters.enabled=true # Example ORC Bloom Filter enablement (from Hive context, applicable concept)
        
    # Maintenance Defaults (Important to understand minimums)
    iceberg.expire-snapshots.min-retention = 7d
    iceberg.remove-orphan-files.min-retention = 7d

    # Security Configuration
    iceberg.security=system
    # iceberg.security=read_only ## When connecting to Databricks using REST Catalog

    # MV configuration
    # materialized-views.enabled=true
    # materialized-views.namespace=catalog_hive_namespace
    # materialized-views.storage-schema=mv_cache_storage
